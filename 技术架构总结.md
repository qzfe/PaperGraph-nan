# 论文知识图谱系统 - 技术架构总结

## 📋 目录

1. [整体架构设计](#整体架构设计)
2. [数据存储层](#数据存储层)
3. [数据访问层](#数据访问层)
4. [业务逻辑层](#业务逻辑层)
5. [API接口层](#api接口层)
6. [异步任务层](#异步任务层)
7. [缓存优化层](#缓存优化层)
8. [配置管理](#配置管理)
9. [日志系统](#日志系统)
10. [技术栈总结](#技术栈总结)

---

## 1. 整体架构设计

### 架构模式：分层架构（Layered Architecture）

```
┌─────────────────────────────────────────┐
│         API Layer (FastAPI)             │  ← HTTP请求入口
├─────────────────────────────────────────┤
│      Service Layer (业务逻辑)           │  ← 业务处理
├─────────────────────────────────────────┤
│   Repository Layer (数据访问 DAO)       │  ← 数据访问抽象
├─────────────────────────────────────────┤
│   Data Layer (MySQL + Neo4j + Redis)    │  ← 数据持久化
└─────────────────────────────────────────┘

         ┌──────────────────┐
         │  Celery Tasks    │  ← 异步任务处理
         └──────────────────┘
```

### 核心设计原则

- **关注点分离**: API、业务逻辑、数据访问各司其职
- **依赖注入**: 使用 FastAPI 的 `Depends` 实现依赖注入
- **单一职责**: 每个模块只负责一个功能领域
- **开闭原则**: 对扩展开放，对修改关闭

---

## 2. 数据存储层

### 2.1 MySQL（关系型数据库）

**技术**: MySQL 8.0+ + SQLAlchemy 2.0.23

**作用**: 存储结构化数据和元数据

**设计模式**: ORM (Object-Relational Mapping)

#### 表结构设计

```python
# 文件: app/models/mysql_models.py

class PaperInfo(Base):
    """论文信息表"""
    __tablename__ = "paper_info"
    paper_id = Column(String(64), primary_key=True)
    title = Column(String(512), nullable=False)
    abstract = Column(Text)
    year = Column(Integer)
    venue = Column(String(128))
    doi = Column(String(128))
    keywords = Column(String(512))
    citation_count = Column(Integer, default=0)
    # ... 时间戳字段
```

**技术特点**:
- 使用 SQLAlchemy ORM 定义模型
- 自动时间戳 (`server_default=func.now()`)
- 外键关系管理 (`ForeignKey`)
- 支持事务（ACID 特性）

#### 存储的数据类型

1. **paper_info** - 论文基本信息
2. **author_info** - 作者信息及指标
3. **organization_info** - 机构信息
4. **paper_author_relation** - 论文-作者多对多关系
5. **paper_citation_relation** - 论文引用关系
6. **graph_node_mapping** - Neo4j 节点映射
7. **statistics_data** - 预计算统计数据（JSON 字段）
8. **export_log** - 导出任务日志

**使用场景**:
- 复杂查询（JOIN、聚合）
- 数据统计分析
- 事务性操作
- 数据完整性约束

---

### 2.2 Neo4j（图数据库）

**技术**: Neo4j 5.14+ + Neo4j Python Driver

**作用**: 存储和查询图谱关系

**查询语言**: Cypher

#### 图模型设计

```cypher
# 节点类型 (Node Labels)
(:Paper)         - 论文节点
(:Author)        - 作者节点
(:Organization)  - 机构节点

# 关系类型 (Relationship Types)
(:Author)-[:AUTHORED]->(:Paper)              # 作者撰写论文
(:Author)-[:AFFILIATED_WITH]->(:Organization) # 作者隶属机构
(:Paper)-[:CITES]->(:Paper)                  # 论文引用关系
```

**技术实现**:

```python
# 文件: app/repositories/neo4j_dao.py

class GraphDAO:
    def query_root(self, params: Dict[str, Any]):
        """查询根节点图谱"""
        query = """
        MATCH (a:Author)-[r1:AUTHORED]->(p:Paper)
        OPTIONAL MATCH (a)-[r2:AFFILIATED_WITH]->(o:Organization)
        WITH a, p, o, r1, r2
        LIMIT $limit
        RETURN a, p, o, r1, r2
        """
        result = self.session.run(query, limit=params.get("limit", 100))
        # 处理节点和边...
```

**性能优化**:
- 索引：在 `id`, `name`, `title` 等字段建立索引
- 约束：唯一性约束确保数据一致性
- 图算法：利用 Neo4j 原生图遍历性能

**使用场景**:
- 图谱可视化（节点展开/折叠）
- 关系查询（N度关系查找）
- 路径查询（引用链路分析）
- 社区发现（作者合作网络）

---

### 2.3 Redis（缓存数据库）

**技术**: Redis 7.0+ + redis-py 5.0.1

**作用**: 缓存热点数据，消息队列

**数据结构**: String (主要)

#### 缓存策略

```python
# 文件: app/services/graph_service.py

class GraphService:
    def get_root(self, params: Dict[str, Any]):
        # 1. 生成缓存键
        cache_key = f"graph:root:{json.dumps(params, sort_keys=True)}"
        
        # 2. 尝试从缓存读取
        if self.cache:
            cached = self.cache.get(cache_key)
            if cached:
                return json.loads(cached)
        
        # 3. 缓存未命中，查询数据库
        nodes, edges = self.dao.query_root(params)
        result = {"nodes": nodes, "edges": edges}
        
        # 4. 写入缓存（TTL: 3600秒）
        if self.cache:
            self.cache.setex(cache_key, 3600, json.dumps(result))
        
        return result
```

**缓存键设计**:
- `graph:root:*` - 根节点图谱数据
- `graph:children:{node_id}` - 子节点数据
- `graph:node:{node_id}` - 节点详情
- `stat:{hash}` - 统计查询结果

**缓存策略**: Cache-Aside Pattern（旁路缓存）

**TTL 设置**:
- 图谱数据: 3600秒（1小时）
- 子节点数据: 1800秒（30分钟）
- 统计数据: 3600秒（1小时）

**使用场景**:
- 频繁查询的图谱数据
- 统计结果缓存
- Celery 消息队列
- 会话存储（未来扩展）

---

## 3. 数据访问层 (DAO/Repository)

### 设计模式: Repository Pattern（仓储模式）

**作用**: 封装数据访问逻辑，提供统一接口

### 3.1 MySQL DAO

**文件**: `app/repositories/mysql_dao.py`

#### StatisticsDAO - 统计数据访问

```python
class StatisticsDAO:
    def __init__(self, db: Session):
        self.db = db
    
    def query_aggregated(self, query_params: Dict[str, Any]):
        """聚合查询"""
        metric = query_params.get("metric")
        
        if metric == "paper_count_by_year":
            # SQLAlchemy 聚合查询
            query = self.db.query(
                PaperInfo.year,
                func.count(PaperInfo.paper_id).label("count")
            ).group_by(PaperInfo.year)
            
            # 动态过滤条件
            if start_year:
                query = query.filter(PaperInfo.year >= start_year)
            
            results = query.all()
            return [{"label": str(r.year), "value": r.count} for r in results]
```

**技术特点**:
- SQLAlchemy 查询构造器
- 动态条件构建
- 聚合函数 (`count`, `sum`, `avg`)
- 关联查询优化

#### ExportDAO - 导出任务管理

```python
class ExportDAO:
    def insert_job(self, job_id: str, user_id: int, params: Dict):
        """插入导出任务"""
        job = ExportLog(job_id=job_id, user_id=user_id, params=params)
        self.db.add(job)
        self.db.commit()  # 事务提交
    
    def update_status(self, job_id: str, status: str, file_path: str):
        """更新任务状态"""
        job = self.get_job(job_id)
        if job:
            job.status = status
            job.file_path = file_path
            self.db.commit()
```

**技术特点**:
- 事务管理
- 异常回滚
- CRUD 操作封装

---

### 3.2 Neo4j DAO

**文件**: `app/repositories/neo4j_dao.py`

#### GraphDAO - 图数据访问

```python
class GraphDAO:
    def query_children(self, node_id: str):
        """查询子节点 - 一度关系"""
        query = """
        MATCH (n)-[r]-(m)
        WHERE id(n) = $node_id
        RETURN n, r, m
        """
        result = self.session.run(query, node_id=int(node_id))
        
        # 处理返回的图数据
        nodes = []
        edges = []
        for record in result:
            # 节点去重
            if record["n"]:
                nodes.append({
                    "id": str(record["n"].id),
                    "label": list(record["n"].labels)[0],
                    "properties": dict(record["n"])
                })
            # 边数据
            if record["r"]:
                edges.append({
                    "id": str(record["r"].id),
                    "source": str(record["r"].start_node.id),
                    "target": str(record["r"].end_node.id),
                    "type": record["r"].type
                })
        
        return nodes, edges
```

**Cypher 查询技巧**:
- `MATCH` - 模式匹配
- `WHERE` - 条件过滤
- `OPTIONAL MATCH` - 可选匹配（类似 LEFT JOIN）
- `WITH` - 中间结果传递
- `LIMIT` - 结果限制

**节点创建**:

```python
def create_paper_node(self, paper_data: Dict):
    """创建论文节点"""
    query = """
    CREATE (p:Paper $properties)
    RETURN id(p) as node_id
    """
    result = self.session.run(query, properties=paper_data)
    return str(result.single()["node_id"])
```

**关系创建**:

```python
def create_relationship(self, start_id: str, end_id: str, rel_type: str):
    """创建关系"""
    query = f"""
    MATCH (a), (b)
    WHERE id(a) = $start_id AND id(b) = $end_id
    CREATE (a)-[r:{rel_type}]->(b)
    RETURN r
    """
    self.session.run(query, start_id=int(start_id), end_id=int(end_id))
```

---

## 4. 业务逻辑层 (Service)

### 设计模式: Service Pattern（服务层模式）

**作用**: 实现业务逻辑，协调多个 DAO

### 4.1 GraphService - 图谱业务

**文件**: `app/services/graph_service.py`

```python
class GraphService:
    def __init__(self, graph_dao: GraphDAO, cache: Redis):
        self.dao = graph_dao      # 数据访问层
        self.cache = cache        # 缓存层
    
    def get_root(self, params: Dict):
        """获取根节点图谱（带缓存）"""
        # 1. 缓存层处理
        cache_key = f"graph:root:{json.dumps(params, sort_keys=True)}"
        cached = self.cache.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # 2. 数据访问层
        nodes, edges = self.dao.query_root(params)
        result = {"nodes": nodes, "edges": edges}
        
        # 3. 写入缓存
        self.cache.setex(cache_key, 3600, json.dumps(result))
        return result
    
    def persist_layout(self, layout_data: List[Dict]):
        """保存布局（清除相关缓存）"""
        success = self.dao.save_layout(layout_data)
        
        # 清除缓存
        if success and self.cache:
            keys = self.cache.keys("graph:root:*")
            if keys:
                self.cache.delete(*keys)
        
        return success
```

**技术特点**:
- 缓存穿透保护
- 缓存失效策略（主动清除）
- 异常处理和日志记录

---

### 4.2 StatisticsService - 统计业务

**文件**: `app/services/statistics_service.py`

```python
class StatisticsService:
    def query_statistics(self, query_params: Dict):
        """查询统计数据"""
        # 1. 生成缓存键（使用 MD5 哈希）
        cache_key = self._generate_cache_key(query_params)
        
        # 2. 缓存查询
        if self.cache:
            cached = self.cache.get(cache_key)
            if cached:
                return json.loads(cached)
        
        # 3. 数据库查询
        data = self.dao.query_aggregated(query_params)
        result = {
            "metric": query_params.get("metric"),
            "data": data,
            "total": len(data)
        }
        
        # 4. 写入缓存
        self.cache.setex(cache_key, 3600, json.dumps(result))
        return result
    
    def _generate_cache_key(self, query_params: Dict) -> str:
        """生成缓存键"""
        key_str = json.dumps(query_params, sort_keys=True)
        key_hash = hashlib.md5(key_str.encode()).hexdigest()
        return f"stat:{key_hash}"
```

**缓存键优化**: 使用 MD5 避免键过长

---

### 4.3 ExportService - 导出业务

**文件**: `app/services/export_service.py`

```python
class ExportService:
    def create_job(self, user_id: int, export_params: Dict):
        """创建导出任务"""
        # 1. 生成唯一任务ID
        job_id = f"exp_{uuid.uuid4().hex[:12]}"
        
        # 2. 插入任务记录
        success = self.dao.insert_job(job_id, user_id, export_params)
        
        # 3. 触发异步任务（由 Celery 处理）
        # 实际触发在 API 层通过 BackgroundTasks
        
        return {
            "job_id": job_id,
            "status": "pending",
            "message": "导出任务已创建"
        }
```

**任务状态管理**:
- `pending` - 待处理
- `running` - 执行中
- `done` - 已完成
- `failed` - 失败

---

## 5. API接口层

### 技术: FastAPI 0.104.1

**特点**:
- 自动生成 OpenAPI 文档
- 类型验证（Pydantic）
- 异步支持
- 依赖注入

### 5.1 API 结构

```
app/api/v1/
├── __init__.py
├── graph.py          # 图谱接口
├── statistics.py     # 统计接口
└── export.py         # 导出接口
```

### 5.2 请求/响应模型 (Pydantic Schema)

**文件**: `app/schemas/`

```python
# graph.py - 图谱 Schema
class NodeSchema(BaseModel):
    id: str = Field(..., description="节点ID")
    label: str = Field(..., description="节点类型")
    properties: Dict[str, Any] = Field(default_factory=dict)

class GraphResponse(BaseModel):
    nodes: List[NodeSchema]
    edges: List[EdgeSchema]

# statistics.py - 统计 Schema
class StatisticsQueryRequest(BaseModel):
    metric: str = Field(..., description="指标名称")
    start_year: Optional[int] = None
    end_year: Optional[int] = None
    limit: Optional[int] = Field(100, ge=1, le=1000)

class StatisticsDataPoint(BaseModel):
    label: str
    value: float
    extra: Optional[Dict[str, Any]] = None
```

**验证功能**:
- 类型检查
- 范围验证 (`ge`, `le`)
- 必填/可选字段
- 默认值设置

---

### 5.3 依赖注入

```python
# 文件: app/api/v1/graph.py

def get_graph_service():
    """依赖注入：创建 GraphService 实例"""
    neo4j_session = next(get_neo4j_session())
    redis_client = get_redis_client()
    dao = GraphDAO(neo4j_session)
    return GraphService(dao, redis_client)

@router.get("/root", response_model=GraphResponse)
async def get_root_graph(
    limit: int = Query(100, ge=1, le=1000),
    service: GraphService = Depends(get_graph_service)  # 依赖注入
):
    """获取根节点图谱"""
    result = service.get_root({"limit": limit})
    return GraphResponse(
        nodes=[NodeSchema(**node) for node in result["nodes"]],
        edges=[EdgeSchema(**edge) for edge in result["edges"]]
    )
```

**依赖注入优势**:
- 解耦代码
- 便于测试
- 自动资源管理

---

### 5.4 异常处理

```python
# 文件: app/main.py

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """全局异常处理"""
    logger.error(f"全局异常: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "code": 500,
            "message": "服务器内部错误",
            "detail": str(exc) if settings.DEBUG else "请联系管理员"
        }
    )
```

---

## 6. 异步任务层

### 技术: Celery 5.3+ + Redis

**作用**: 处理耗时任务（文件导出）

### 6.1 Celery 配置

**文件**: `app/tasks/celery_app.py`

```python
celery_app = Celery(
    "paper_kg",
    broker=settings.CELERY_BROKER_URL,      # redis://localhost:6379/1
    backend=settings.CELERY_RESULT_BACKEND,  # redis://localhost:6379/2
    include=["app.tasks.export_tasks"]
)

celery_app.conf.update(
    task_serializer="json",          # 任务序列化格式
    accept_content=["json"],
    result_serializer="json",
    timezone="Asia/Shanghai",
    enable_utc=True,
    task_track_started=True,         # 跟踪任务启动
    task_time_limit=3600,            # 任务超时（1小时）
    task_soft_time_limit=3000,       # 软超时
    worker_prefetch_multiplier=1,    # 预取任务数
)
```

---

### 6.2 导出任务实现

**文件**: `app/tasks/export_tasks.py`

```python
@celery_app.task(bind=True, name="export.papers")
def export_papers_task(self, job_id: str, params: Dict):
    """导出论文数据（异步任务）"""
    try:
        # 1. 更新状态为 running
        dao.update_status(job_id, "running")
        
        # 2. 查询数据
        papers = db.query(PaperInfo).all()
        
        # 3. 生成文件
        file_format = params.get("format", "csv")
        if file_format == "csv":
            file_path = export_to_csv(papers, job_id)
        elif file_format == "excel":
            file_path = export_to_excel(papers, job_id)
        
        # 4. 更新状态为 done
        dao.update_status(job_id, "done", file_path=file_path)
        
    except Exception as e:
        # 失败处理
        dao.update_status(job_id, "failed", error_msg=str(e))
        raise
```

**任务特性**:
- `bind=True` - 绑定任务实例（访问 self）
- 异常自动重试（可配置）
- 进度跟踪
- 结果存储

---

### 6.3 任务触发

```python
# 文件: app/api/v1/export.py

@router.post("/file", response_model=ExportJobResponse)
async def create_export_job(
    request: ExportRequest,
    background_tasks: BackgroundTasks,  # FastAPI 后台任务
    service: ExportService = Depends(get_export_service)
):
    # 创建任务记录
    result = service.create_job(user_id=None, export_params=request.dict())
    
    # 触发 Celery 任务
    export_papers_task.delay(result["job_id"], request.dict())
    
    return ExportJobResponse(**result)
```

---

## 7. 缓存优化层

### 7.1 缓存策略

**Cache-Aside Pattern（旁路缓存）**

```python
def get_data_with_cache(key: str):
    # 1. 读取缓存
    data = cache.get(key)
    if data:
        return json.loads(data)
    
    # 2. 缓存未命中，查询数据库
    data = database.query(...)
    
    # 3. 写入缓存
    cache.setex(key, TTL, json.dumps(data))
    
    return data
```

### 7.2 缓存失效策略

**主动失效（Write-Through）**:

```python
def update_data(data):
    # 1. 更新数据库
    database.update(data)
    
    # 2. 删除相关缓存
    cache.delete(f"data:{data.id}")
    cache.delete_pattern("data:list:*")  # 删除列表缓存
```

### 7.3 缓存键命名规范

```
graph:root:{params_hash}           # 根节点图谱
graph:children:{node_id}           # 子节点数据
graph:node:{node_id}               # 节点详情
stat:{query_hash}                  # 统计查询结果
```

---

## 8. 配置管理

### 技术: Pydantic Settings

**文件**: `config.py`

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # 应用配置
    APP_NAME: str = "论文知识图谱系统"
    DEBUG: bool = True
    
    # MySQL 配置
    MYSQL_HOST: str = "localhost"
    MYSQL_PASSWORD: str = ""
    
    # 动态属性
    @property
    def mysql_url(self) -> str:
        return f"mysql+pymysql://{self.MYSQL_USER}:{self.MYSQL_PASSWORD}@{self.MYSQL_HOST}:{self.MYSQL_PORT}/{self.MYSQL_DATABASE}"
    
    class Config:
        env_file = ".env"          # 从 .env 文件读取
        case_sensitive = True       # 区分大小写
```

**特点**:
- 类型验证
- 环境变量支持
- 默认值设置
- 动态计算属性

---

## 9. 日志系统

### 技术: Loguru

**文件**: `app/main.py`

```python
from loguru import logger

# 配置日志
logger.remove()  # 移除默认处理器

# 控制台输出
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | {message}",
    level="DEBUG" if settings.DEBUG else "INFO"
)

# 文件输出
logger.add(
    "logs/app_{time:YYYY-MM-DD}.log",
    rotation="00:00",      # 每天轮转
    retention="30 days",   # 保留30天
    level="INFO"
)
```

**日志级别**:
- `DEBUG` - 详细调试信息
- `INFO` - 一般信息
- `WARNING` - 警告
- `ERROR` - 错误
- `CRITICAL` - 严重错误

**使用示例**:

```python
logger.info(f"查询根节点: limit={limit}")
logger.error(f"数据库连接失败: {e}")
logger.warning(f"缓存读取失败，降级到数据库查询")
```

---

## 10. 技术栈总结

### 后端框架
- **FastAPI 0.104.1** - 现代异步 Web 框架
- **Uvicorn** - ASGI 服务器

### 数据库
- **MySQL 8.0+** - 关系型数据库
- **Neo4j 5.14+** - 图数据库
- **Redis 7.0+** - 内存数据库（缓存+队列）

### ORM & 驱动
- **SQLAlchemy 2.0.23** - Python ORM
- **PyMySQL 1.1.0** - MySQL 驱动
- **neo4j 5.14.1** - Neo4j Python 驱动
- **redis-py 5.0.1** - Redis 客户端

### 异步任务
- **Celery 5.3.4** - 分布式任务队列
- **Flower 2.0.1** - Celery 监控工具

### 数据处理
- **Pandas 2.1.3** - 数据处理
- **openpyxl 3.1.2** - Excel 文件处理

### 工具库
- **Pydantic 2.5.0** - 数据验证
- **Loguru 0.7.2** - 日志记录
- **python-dotenv 1.0.0** - 环境变量管理

---

## 技术架构优势

### 1. 高性能
- Redis 缓存减少数据库压力
- Neo4j 原生图遍历性能优异
- FastAPI 异步处理提升并发能力

### 2. 高可扩展
- 分层架构便于扩展
- 微服务化潜力（各层可独立部署）
- 支持水平扩展（Celery Worker、Redis Cluster）

### 3. 高可维护
- 代码分层清晰
- 类型提示完善
- 文档自动生成

### 4. 高可靠
- 事务支持保证数据一致性
- 异常处理机制完善
- 日志系统便于问题排查

---

## 性能优化策略

### 数据库层
1. **索引优化**: 在常用查询字段建立索引
2. **查询优化**: 避免 N+1 查询，使用 JOIN
3. **连接池**: SQLAlchemy 连接池管理

### 缓存层
1. **多级缓存**: Redis（热数据）+ 本地缓存（超热数据）
2. **缓存预热**: 系统启动时预加载常用数据
3. **懒加载**: 按需加载数据

### API层
1. **异步处理**: FastAPI 异步路由
2. **请求限流**: 防止恶意请求
3. **响应压缩**: Gzip 压缩减少传输

### 任务层
1. **任务优先级**: Celery 队列优先级
2. **并发控制**: Worker 并发数配置
3. **失败重试**: 自动重试机制

---

## 总结

这个项目采用了**现代化的技术栈**和**成熟的架构模式**，具有以下特点：

1. **技术选型合理**: FastAPI + Neo4j + MySQL + Redis 完美结合
2. **架构设计优秀**: 分层清晰，职责明确
3. **代码质量高**: 类型提示、异常处理、日志记录完善
4. **性能优异**: 缓存、异步、图数据库优化
5. **易于维护**: 模块化设计，文档完善

是一个**生产级别**的企业级应用架构！🎯

